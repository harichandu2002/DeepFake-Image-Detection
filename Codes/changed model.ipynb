{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a15b86f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aab27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Path of Dataset\n",
    "BASE_PATH = r\"C:\\DeefFake Project\\431 MB\"\n",
    "TRAIN_REAL_DIR = os.path.join(BASE_PATH,\"Train\",\"training_real\")\n",
    "TRAIN_FAKE_DIR = os.path.join(BASE_PATH, \"Train\",\"training_fake\")\n",
    "TEST_REAL_DIR = os.path.join(BASE_PATH, \"Test\", \"Real\")\n",
    "TEST_FAKE_DIR = os.path.join(BASE_PATH, \"Test\", \"Fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1b3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Haar Cascade\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6419d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(input_real_dir, input_fake_dir, output_dir):\n",
    "    \n",
    "    processed_real_dir = os.path.join(output_dir, \"real\")\n",
    "    processed_fake_dir = os.path.join(output_dir, \"fake\")\n",
    "    if not os.path.exists(processed_real_dir):\n",
    "        os.makedirs(processed_real_dir)\n",
    "    if not os.path.exists(processed_fake_dir):\n",
    "        os.makedirs(processed_fake_dir)\n",
    "\n",
    "    # Process real images\n",
    "    real_image_paths = glob(os.path.join(input_real_dir, '*.jpg'))\n",
    "    for img_path in tqdm(real_image_paths, desc=f\"Cropping real faces from {input_real_dir}\"):\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None: continue\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]\n",
    "            face_img = img[y:y+h, x:x+w]\n",
    "            output_path = os.path.join(processed_real_dir, os.path.basename(img_path))\n",
    "            cv2.imwrite(output_path, face_img)\n",
    "    \n",
    "    # Process fake images\n",
    "    fake_image_paths = glob(os.path.join(input_fake_dir, '*.jpg'))\n",
    "    for img_path in tqdm(fake_image_paths, desc=f\"Cropping fake faces from {input_fake_dir}\"):\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None: continue\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]\n",
    "            face_img = img[y:y+h, x:x+w]\n",
    "            output_path = os.path.join(processed_fake_dir, os.path.basename(img_path))\n",
    "            cv2.imwrite(output_path, face_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d84e2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(input_real_dir, input_fake_dir, output_dir):\n",
    "    processed_real_dir = os.path.join(output_dir, \"real\")\n",
    "    processed_fake_dir = os.path.join(output_dir, \"fake\")\n",
    "    os.makedirs(processed_real_dir, exist_ok=True)\n",
    "    os.makedirs(processed_fake_dir, exist_ok=True)\n",
    "\n",
    "    saved_real = 0\n",
    "    saved_fake = 0\n",
    "\n",
    "    # Process real images\n",
    "    real_image_paths = glob(os.path.join(input_real_dir, '*.jpg'))\n",
    "    for img_path in tqdm(real_image_paths, desc=f\"Processing real images\"):\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]\n",
    "            face_img = img[y:y+h, x:x+w]\n",
    "        else:\n",
    "            face_img = img  # fallback: use full image\n",
    "\n",
    "        output_path = os.path.join(processed_real_dir, os.path.basename(img_path))\n",
    "        cv2.imwrite(output_path, face_img)\n",
    "        saved_real += 1\n",
    "\n",
    "    # Process fake images\n",
    "    fake_image_paths = glob(os.path.join(input_fake_dir, '*.jpg'))\n",
    "    for img_path in tqdm(fake_image_paths, desc=f\"Processing fake images\"):\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]\n",
    "            face_img = img[y:y+h, x:x+w]\n",
    "        else:\n",
    "            face_img = img  # fallback: use full image\n",
    "\n",
    "        output_path = os.path.join(processed_fake_dir, os.path.basename(img_path))\n",
    "        cv2.imwrite(output_path, face_img)\n",
    "        saved_fake += 1\n",
    "\n",
    "    # Logging\n",
    "    print(f\"\\n✅ Saved {saved_real} real images to {processed_real_dir}\")\n",
    "    print(f\"✅ Saved {saved_fake} fake images to {processed_fake_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e46ff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f1ee7e9a5c45ff86dc36d92ec57687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing real images:   0%|          | 0/16081 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bfb58e02674ad188d079fccbbe00fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing fake images:   0%|          | 0/15960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved 16081 real images to data/processed1\\train\\real\n",
      "✅ Saved 15960 fake images to data/processed1\\train\\fake\n",
      "\n",
      "Preprocessing testing images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b79bb3dff364fdb839b994fb3f8cec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing real images:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313a18db3c4d47b9959915b29f6da086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing fake images:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved 5000 real images to data/processed1\\test\\real\n",
      "✅ Saved 5000 fake images to data/processed1\\test\\fake\n"
     ]
    }
   ],
   "source": [
    "# Creating directories for preprocessed data\n",
    "PROCESSED_DATA_DIR = 'data/processed1'\n",
    "PROCESSED_TRAIN_DIR = os.path.join(PROCESSED_DATA_DIR, \"train\")\n",
    "PROCESSED_TEST_DIR = os.path.join(PROCESSED_DATA_DIR, \"test\")\n",
    "\n",
    "print(\"Preprocessing training images...\")\n",
    "preprocess_images(TRAIN_REAL_DIR, TRAIN_FAKE_DIR, PROCESSED_TRAIN_DIR)\n",
    "print(\"\\nPreprocessing testing images...\")\n",
    "preprocess_images(TEST_REAL_DIR, TEST_FAKE_DIR, PROCESSED_TEST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "702661ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFakeDataset(Dataset):\n",
    "    def __init__(self, real_dir, fake_dir, transform=None):\n",
    "        self.real_files = glob(os.path.join(real_dir, '*.jpg'))\n",
    "        self.fake_files = glob(os.path.join(fake_dir, '*.jpg'))\n",
    "        self.files = self.real_files + self.fake_files\n",
    "        self.labels = [0] * len(self.real_files) + [1] * len(self.fake_files)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec535112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining image transformations\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # XceptionNet requires 299x299\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6e747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the datasets\n",
    "train_dataset = DeepFakeDataset(os.path.join(PROCESSED_TRAIN_DIR, \"real\"), os.path.join(PROCESSED_TRAIN_DIR, \"fake\"), transform=image_transforms)\n",
    "test_dataset = DeepFakeDataset(os.path.join(PROCESSED_TEST_DIR, \"real\"), os.path.join(PROCESSED_TEST_DIR, \"fake\"), transform=image_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e4542c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 32041\n",
      "Number of testing samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# Creating DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c18324fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VS Code\\Jupyter Notebook\\src\\virenv\\Lib\\site-packages\\timm\\models\\_factory.py:138: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and configured successfully.\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = timm.create_model('xception', pretrained=True)\n",
    "\n",
    "# Modifying the final layer for binary classification\n",
    "num_ftrs = model.get_classifier().in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model loaded and configured successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce257cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48032889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f74755ca324421a98c92376d9e079f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1002 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1828\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a374077f47324455ab094baac60a1112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1002 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1007\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50e4b6baf3041a7b1ce3482c2bd05d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1002 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0814\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a31fcaf43643febe2551943033244b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1002 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0753\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d648cdc8df4e0cab6f338a68bdcd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1002 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0683\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "best_val_acc = 0.0\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.float().to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de40216e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d70fea7fbf426d9710c804852b21a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluating model\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_labels_list = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = images.to(device), labels.float().to(device).unsqueeze(1)\n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels_list.extend(labels.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "161c5ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Set Performance ---\n",
      "Test Accuracy: 0.9902\n",
      "Test Precision: 0.9906\n",
      "Test Recall: 0.9898\n",
      "Test F1-Score: 0.9902\n"
     ]
    }
   ],
   "source": [
    "# Calculating test metrics\n",
    "test_preds_binary = [1 if p > 0.5 else 0 for p in test_preds]\n",
    "test_acc = accuracy_score(test_labels_list, test_preds_binary)\n",
    "test_precision = precision_score(test_labels_list, test_preds_binary)\n",
    "test_recall = recall_score(test_labels_list, test_preds_binary)\n",
    "test_f1 = f1_score(test_labels_list, test_preds_binary)\n",
    "\n",
    "print(\"\\n--- Test Set Performance ---\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "def predict_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return \"Error: Could not read image.\", 0.0\n",
    "        \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return \"No face detected.\", 0.0\n",
    "    \n",
    "    (x, y, w, h) = faces[0]\n",
    "    face_img = img[y:y+h, x:x+w]\n",
    "    \n",
    "    # Converting back to PIL Image and apply transformations\n",
    "    face_img_pil = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
    "    input_tensor = image_transforms(face_img_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    # Getting prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        prediction = torch.sigmoid(output).item()\n",
    "\n",
    "    if prediction > 0.5:\n",
    "        return \"Prediction: Fake\", prediction\n",
    "    else:\n",
    "        return \"Prediction: Real\", prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54b88016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Xception(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act2): ReLU(inplace=True)\n",
       "  (block1): Block(\n",
       "    (skip): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (skipbn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (rep): Sequential(\n",
       "      (0): SeparableConv2d(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "        (pointwise): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): SeparableConv2d(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block2): Block(\n",
       "    (skip): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (skipbn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (pointwise): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block3): Block(\n",
       "    (skip): Conv2d(256, 728, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (skipbn): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "        (pointwise): Conv2d(256, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block4): Block(\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block5): Block(\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block6): Block(\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block7): Block(\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block8): Block(\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block9): Block(\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block10): Block(\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block11): Block(\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block12): Block(\n",
       "    (skip): Conv2d(728, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (skipbn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (conv3): SeparableConv2d(\n",
       "    (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "    (pointwise): Conv2d(1024, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (bn3): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act3): ReLU(inplace=True)\n",
       "  (conv4): SeparableConv2d(\n",
       "    (conv1): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "    (pointwise): Conv2d(1536, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (bn4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act4): ReLU(inplace=True)\n",
       "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6f46e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved successfully as 'deepfake_xception_model.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"deepfake_xception_model.pth\")\n",
    "print(\"✅ Model saved successfully as 'deepfake_xception_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97e3713a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model weights saved as 'deepfake_model1.pt'\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"deepfake_model1.pt\")\n",
    "print(\"✅ Model weights saved as 'deepfake_model1.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3022111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VS Code\\Jupyter Notebook\\src\\virenv\\Lib\\site-packages\\timm\\models\\_factory.py:138: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded and ready for inference.\n"
     ]
    }
   ],
   "source": [
    "#To Load the model\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Recreate the model architecture\n",
    "model = timm.create_model('xception', pretrained=False)\n",
    "num_ftrs = model.get_classifier().in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "# Load the saved weights\n",
    "model.load_state_dict(torch.load(\"deepfake_xception_model.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Model loaded and ready for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d534f702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Real\n",
      "Confidence: 0.3762\n"
     ]
    }
   ],
   "source": [
    "result, confidence = predict_image(r\"C:\\Users\\T8664\\Downloads\\Real time images\\20250117_222808.jpg\")\n",
    "print(result)\n",
    "print(f\"Confidence: {confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3798ef49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VS Code\\Jupyter Notebook\\src\\virenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\VS Code\\Jupyter Notebook\\src\\virenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\T8664/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:30<00:00, 1.56MB/s]\n",
      "C:\\Users\\T8664\\AppData\\Local\\Temp\\ipykernel_17912\\719472180.py:5: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "torch.save(quantized_model.state_dict(), \"quantized_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fccf4e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_file(file_path, chunk_size_mb=25):\n",
    "    chunk_size = chunk_size_mb * 1024 * 1024\n",
    "    base_name = os.path.basename(file_path)\n",
    "    dir_name = os.path.dirname(file_path)\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk_filename = os.path.join(dir_name, f\"{base_name}.part{idx+1}\")\n",
    "        with open(chunk_filename, 'wb') as chunk_file:\n",
    "            chunk_file.write(chunk)\n",
    "\n",
    "    print(f\"Split {base_name} into {len(chunks)} parts under {chunk_size_mb}MB each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cee3562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed model size: 39.98 MB\n",
      "⚠️ Compression not sufficient: File still exceeds 25MB.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "model_path = \"C:/VS Code/Jupyter Notebook/quantized_model.pth\"\n",
    "compressed_path = \"C:/VS Code/Jupyter Notebook/quantized_model_compressed.zip\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(compressed_path), exist_ok=True)\n",
    "\n",
    "# === Compress the model file ===\n",
    "with zipfile.ZipFile(compressed_path, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(model_path, arcname=os.path.basename(model_path))\n",
    "\n",
    "# === Check compressed size ===\n",
    "compressed_size_mb = os.path.getsize(compressed_path) / (1024 * 1024)\n",
    "print(f\"Compressed model size: {compressed_size_mb:.2f} MB\")\n",
    "\n",
    "if compressed_size_mb <= 25:\n",
    "    print(\"✅ Compression successful: File is under 25MB.\")\n",
    "else:\n",
    "    print(\"⚠️ Compression not sufficient: File still exceeds 25MB.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
